# -*- coding: utf-8 -*-
"""DSN_Hackaton.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zfexWMPiNuWDYLQY5lK2yVBKij2WbaqF
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning


# ==============================
# Imports
# ==============================
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import HistGradientBoostingRegressor
import lightgbm as lgb


# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# Loading Dataset"""

try:
    train_df = pd.read_csv("/kaggle/input/datasets-1/train.csv")
    print("Data loaded successfully!")
except FileNotFoundError:
    print("Error: The file 'car_prices.csv' was not found. Please check the file path.")

"""# Exploratory Data Analysis"""

def plots(columns,ncols_for_subplot, df):
    ncols = ncols_for_subplot
    nrows = -(-len(columns) // ncols)  # ceiling division

    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 5 * nrows))
    axes = axes.flatten()

    for i, col in enumerate(columns):
        sns.scatterplot(x=col, y='price', data=df, ax=axes[i])
        axes[i].set_title(f'Price vs. {col}')
        axes[i].set_xlabel(col)
        axes[i].set_ylabel('Price')

    # hide unused subplots if any
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()

categorical_cols = train_df.select_dtypes(include='object').columns
numerical_cols = train_df.select_dtypes(exclude='object').columns
def exploratory_data_analysis(df):
    '''This function performs some preliminary EDA. You are free to add more to it to
       guide you in preparing your dataset for trainiing
    '''
    print("First 5 rows of the dataset:")
    print(df.head())

    # Get information about the dataset (data types, non-null values)
    print("\nDataset information:")
    df.info()

    # Get descriptive statistics for numerical columns
    print("\nDescriptive statistics for numerical columns:")
    print(df.describe())
    #Get descriptive statistics for categorical columns
    print("\nDescriptive statistics for categorical columns:")
    print(df.describe(include='object'))
    #Checking for missing values
    print("\nMissing values per column:")
    print(df.isnull().sum())
    # Visualize the distribution of the target variable (price)
    plt.figure(figsize=(10, 6))
    sns.histplot(df['price'], kde=True, bins=50)
    plt.title('Distribution of Car Prices')
    plt.xlabel('Price')
    plt.ylabel('Frequency')
    plt.show()


    # Visualizing the relationship between all numerical features and price
    # For example, 'mileage' and 'price'
    print('Plotting numerical variables vs price')
    numerical_plot=plots(numerical_cols,2,df)

exploratory_data_analysis(train_df)

#@title Preprocessing Function

def enhanced_preprocessing(df):
    df = df.copy()
    current_year = 2024

    df['int_col'] = df['int_col'].replace('â€“', np.nan)
    df['accident'] = df['accident'].map({
        'None reported': 0,
        'At least 1 accident or damage reported': 1
    })
    df['clean_title'] = df['clean_title'].map({'Yes': 1, 'No': 0})

    df['engine_displacement'] = (
        df['engine']
        .astype(str)
        .str.extract(r'(\d+\.\d+|\d+)')
        .astype(float)
    )

    df['milage'] = (
        df['milage']
        .astype(str)
        .str.replace('[^0-9]', '', regex=True)
        .replace('', np.nan)
        .astype(float)
    )

    df['model_year'] = pd.to_numeric(df['model_year'], errors='coerce')
    df['transmission'] = df['transmission'].fillna("Unknown")
    df['fuel_type'] = df['fuel_type'].fillna("Unknown")

    df['car_age'] = (current_year - df['model_year']).clip(lower=0)
    df['mileage_per_year'] = df['milage'] / (df['car_age'] + 1)

    luxury_brands = ['BMW', 'Mercedes-Benz', 'Audi', 'Lexus', 'Porsche', 'Jaguar', 'Land Rover', 'Volvo']
    df['luxury_brand'] = df['brand'].isin(luxury_brands).astype(int)
    df['high_mileage'] = (df['milage'] > df['milage'].quantile(0.9)).astype(int)

    df['engine_size_category'] = pd.cut(
        df['engine_displacement'],
        bins=[0, 1.5, 2.5, 4.0, float('inf')],
        labels=['Small', 'Medium', 'Large', 'Very Large']
    ).astype(str).fillna('Unknown')

    df['age_mileage_interaction'] = df['car_age'] * df['milage']

    features = [
        'brand', 'ext_col', 'int_col', 'transmission', 'fuel_type', 'engine_size_category',
        'model_year', 'car_age', 'milage', 'mileage_per_year',
        'engine_displacement', 'age_mileage_interaction',
        'accident', 'clean_title', 'luxury_brand', 'high_mileage'
    ]
    return df[features]

# ==============================
#  Full Pipeline with Hyperparameter Tuning
# ==============================
def multi_model_tuned():
    # Load datasets
    train = pd.read_csv("/kaggle/input/datasets-1/train.csv")
    test = pd.read_csv("/kaggle/input/datasets-1/test.csv")

    y = train['price']
    X = enhanced_preprocessing(train)
    X_test = enhanced_preprocessing(test)
    test_ids = test['id']

    # Identify categorical columns
    cat_cols = X.select_dtypes(include='object').columns.tolist()
    print("Categorical columns:", cat_cols)

    # Fix NaNs in categorical columns
    for col in cat_cols:
        X[col] = X[col].fillna("Unknown").astype(str)
        X_test[col] = X_test[col].fillna("Unknown").astype(str)

    # Encode categorical columns for HistGB and LightGBM
    X_enc = X.copy()
    X_test_enc = X_test.copy()
    for col in cat_cols:
        le = LabelEncoder()
        X_enc[col] = le.fit_transform(X_enc[col])
        X_test_enc[col] = le.transform(X_test_enc[col])

    # Split train/validation
    X_train_enc, X_val_enc,y_train, y_val= train_test_split(X_enc, y, test_size=0.2, random_state=42)


    # ===== HistGradientBoosting =====
    hgb_model = HistGradientBoostingRegressor(random_state=42)
    hgb_param_grid = {
        'max_iter': [500, 1000],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [4, 6, 8],
        'max_leaf_nodes': [31, 50, 70],
        'min_samples_leaf': [20, 50, 100]
    }
    hgb_search = RandomizedSearchCV(
        estimator=hgb_model,
        param_distributions=hgb_param_grid,
        n_iter=10,
        scoring='neg_root_mean_squared_error',
        cv=3,
        verbose=1,
        random_state=42
    )
    hgb_search.fit(X_train_enc, y_train)
    hgb_best = hgb_search.best_estimator_
    pred_hgb_val = hgb_best.predict(X_val_enc)
    rmse_hgb = mean_squared_error(y_val, pred_hgb_val) ** 0.5
    print(f"HistGradientBoosting Validation RMSE: {rmse_hgb:.2f}")

    # ===== LightGBM =====
    lgb_model = lgb.LGBMRegressor(random_state=42, n_jobs=-1)
    lgb_param_grid = {
        'n_estimators': [500, 1000],
        'learning_rate': [0.01, 0.05, 0.1],
        'num_leaves': [31, 50, 70],
        'subsample': [0.7, 0.8, 1.0],
        'colsample_bytree': [0.7, 0.8, 1.0]
    }
    lgb_search = RandomizedSearchCV(
        estimator=lgb_model,
        param_distributions=lgb_param_grid,
        n_iter=10,
        scoring='neg_root_mean_squared_error',
        cv=3,
        verbose=1,
        random_state=42
    )
    lgb_search.fit(X_train_enc, y_train)
    lgb_best = lgb_search.best_estimator_
    pred_lgb_val = lgb_best.predict(X_val_enc)
    rmse_lgb = mean_squared_error(y_val, pred_lgb_val) ** 0.5
    print(f" LightGBM Validation RMSE: {rmse_lgb:.2f}")

    # ===== Save all test predictions =====
    pred_hgb_test = hgb_best.predict(X_test_enc)
    pred_lgb_test = lgb_best.predict(X_test_enc)

    pd.DataFrame({'id': test_ids, 'price': pred_hgb_test}).to_csv("submission_hgb.csv", index=False)
    pd.DataFrame({'id': test_ids, 'price': pred_lgb_test}).to_csv("submission_lgb.csv", index=False)
    print("\n Submissions saved for HistGB, and LightGBM!")

#@title RUN

if __name__ == "__main__":
    multi_model_tuned()

